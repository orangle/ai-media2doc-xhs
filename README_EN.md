# AI-Media2Doc Â· Generate Xiaohongshu Posts from Video

AI-Media2Doc is an end-to-end workflow that turns short-form videos into Xiaohongshu-style articles. A lightweight **Streamlit** UI wraps a pure-Python backend pipeline that extracts audio and key frames, calls Qwen3 models on the iFlow platform for speech recognition, visual understanding, fact structuring, and finally produces polished Markdown ready to publish.

- âš–ï¸ MIT licensed and open source â€“ deploy it anywhere.
- ğŸ” Privacy-friendly â€“ temporary files stay on your machine.
- ğŸ§  Unified AI pipeline: ASR â†’ vision â†’ fact extraction â†’ copywriting.
- ğŸš€ Zero-config runtime â€“ start Streamlit and you are ready to create.

---

## Architecture

```
Upload video
      â†“
Media preprocessing (audio extraction & key frames)
      â†“
ASR (local Whisper fallback or Qwen3-Max)
      â†“
Vision grounding (Qwen3-VL-Plus)
      â†“
Structured fact extraction (Qwen3-Max)
      â†“
Xiaohongshu-style writing (Qwen3-Max)
      â†“
Streamlit rendering
```

| Module | Location | Responsibility |
| --- | --- | --- |
| `video_utils.py` | `backend/core` | Extract audio & key frames via `ffmpeg` + `PySceneDetect` |
| `asr.py` | `backend/core` | Run Whisper locally when available, otherwise call iFlow Qwen3-Max |
| `visual_extractor.py` | `backend/core` | Call Qwen3-VL-Plus to describe each frame |
| `fact_extractor.py` | `backend/core` | Summarise ASR + vision outputs into structured travel facts |
| `post_writer.py` | `backend/core` | Generate Xiaohongshu title & Markdown from facts |
| `app/ui.py` | `app` | Streamlit UI: upload, trigger pipeline, render results |

---

## Repository Layout

```
.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ ui.py              # Streamlit entrypoint
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ core/              # Processing pipeline modules
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ TECH_DESIGN.md
â”œâ”€â”€ README.md              # Chinese guide
â””â”€â”€ variables_template.env
```

---

## Getting Started

1. **Prerequisites**
   - Python 3.10+
   - `ffmpeg` available on the system PATH
   - Network access to the iFlow API (`https://api.iflow.cn`)

2. **Install dependencies**

   ```bash
   git clone https://github.com/hanshuaikang/AI-Media2Doc.git
   cd AI-Media2Doc
   python -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Environment variables**

   Create a `.env` or `variables.env` file based on `variables_template.env` and configure:

   | Variable | Description | Default |
   | --- | --- | --- |
   | `IFLOW_API_KEY` | API key for iFlow | required |
   | `IFLOW_API_URL` | Chat Completions endpoint | `https://api.iflow.cn/v1/chat/completions` |
   | `IFLOW_MODEL_ASR` | Model used for speech recognition | `qwen3-max` |
   | `IFLOW_MODEL_VISION` | Model used for frame understanding | `qwen3-vl-plus` |
   | `IFLOW_MODEL_FACT` | Model used for fact extraction | `qwen3-max` |
   | `IFLOW_MODEL_WRITER` | Model used for copywriting | `qwen3-max` |

   > Whisper is optional â€“ if it is not installed, ASR automatically falls back to the Qwen3-Max API call.

---

## Run the App

```bash
streamlit run app/ui.py
```

The terminal prints the URL (default `http://localhost:8501`). Upload a short video and click **Generate** to trigger the pipeline. The result page includes:

- **Xiaohongshu copy** in Markdown â€“ copy & publish directly.
- **Structured facts** â€“ location, budget, activities, transportation, notes, and tags.
- **Frame gallery** â€“ extracted key frames for manual selection.

Example output:

```markdown
# ğŸš¶â€â™€ï¸ Weekend Walk in South Street
- âœ… Must see: Gate plaza at night
- ğŸœ Street food combo: Tofu pudding noodles + grilled cold noodles
- ğŸš‡ Metro Line 2, 5 min walk
```

```json
{
  "åœ°ç‚¹": "åŸå—å¤è¡—",
  "è´¹ç”¨": "äººå‡ 60 å…ƒ",
  "ç©æ³•": ["å¤œæ¸¸è€è¡—", "å“å°è¡—è¾¹å°åƒ"],
  "äº¤é€š": "åœ°é“ 2 å·çº¿åŸå—ç«™",
  "æ—¶é—´": "å‘¨æœ«å‚æ™š",
  "æ³¨æ„äº‹é¡¹": ["æ—©ç‚¹æ’é˜Ÿè´­ä¹°å°åƒ", "å¤œé—´æ³¨æ„ä¿æš–"],
  "æ ‡ç­¾": ["å¤œæ™¯", "å¸‚é›†", "åŸå¸‚æ…¢ç”Ÿæ´»"]
}
```

---

## Contributing & License

Contributions are welcome â€“ feel free to submit issues or pull requests for model prompts, UX, or deployment guides. The project is released under the [MIT License](./LICENSE).
